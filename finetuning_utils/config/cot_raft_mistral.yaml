### model
model_name_or_path: mistralai/Mistral-7B-Instruct-v0.2

### TODO: Parameters that Need to customize
dataset: lora_cot_raft_feedback_score_1_train,Preference-Collection_feedback_score_train
eval_dataset: lora_cot_raft_feedback_score_1_validation
output_dir: saves/mistral-7b/lora/cot_single_and_pair
### The effective batch size should be 4
per_device_train_batch_size: 8
gradient_accumulation_steps: 1
# resume_from_checkpoint: saves/mistral-7b/lora/cot_raft_lora_cot_raft_weight_1e_1/checkpoint-23500

### method
stage: cot_raft
do_train: true
finetuning_type: lora
lora_target: all
deepspeed: examples/deepspeed/ds_z3_config.json

### dataset
template: mistral
cutoff_len: 3000
overwrite_cache: true
preprocessing_num_workers: 16
# max_samples: 100

### output
logging_steps: 10
save_strategy: steps
save_steps: 500
plot_loss: true
overwrite_output_dir: true

### train
learning_rate: 1.0e-5
num_train_epochs: 2.0
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000
enable_liger_kernel: true

### eval
do_eval: false
# eval_strategy: epoch
# per_device_eval_batch_size: 1
